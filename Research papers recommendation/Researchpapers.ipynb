{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>['cs.CV', 'cs.IR']</td>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "      <td>Despite the growing availability of big data i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "      <td>This paper presents a simple end-to-end model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "      <td>The popular Q-learning algorithm is known to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "      <td>Principal components analysis (PCA) is a well-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "      <td>SDYNA is a general framework designed to addre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             terms  \\\n",
       "0                                        ['cs.LG']   \n",
       "1                               ['cs.LG', 'cs.AI']   \n",
       "2                    ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3                               ['cs.LG', 'cs.CR']   \n",
       "4                                        ['cs.LG']   \n",
       "...                                            ...   \n",
       "56176                           ['cs.CV', 'cs.IR']   \n",
       "56177  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
       "56178                                    ['cs.LG']   \n",
       "56179              ['stat.ML', 'cs.LG', 'math.OC']   \n",
       "56180                ['cs.LG', 'cs.AI', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "0      Multi-Level Attention Pooling for Graph Neural...   \n",
       "1      Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2      Power up! Robust Graph Convolutional Network v...   \n",
       "3      Releasing Graph Neural Networks with Different...   \n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "...                                                  ...   \n",
       "56176  Mining Spatio-temporal Data on Industrializati...   \n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...   \n",
       "56178  Deep Reinforcement Learning with Double Q-lear...   \n",
       "56179                        Generalized Low Rank Models   \n",
       "56180  Chi-square Tests Driven Method for Learning th...   \n",
       "\n",
       "                                               abstracts  \n",
       "0      Graph neural networks (GNNs) have been widely ...  \n",
       "1      Deep networks and decision forests (such as ra...  \n",
       "2      Graph convolutional networks (GCNs) are powerf...  \n",
       "3      With the increasing popularity of Graph Neural...  \n",
       "4      Machine learning solutions for pattern classif...  \n",
       "...                                                  ...  \n",
       "56176  Despite the growing availability of big data i...  \n",
       "56177  This paper presents a simple end-to-end model ...  \n",
       "56178  The popular Q-learning algorithm is known to o...  \n",
       "56179  Principal components analysis (PCA) is a well-...  \n",
       "56180  SDYNA is a general framework designed to addre...  \n",
       "\n",
       "[56181 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing data\n",
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "lenght : 1177\n"
     ]
    }
   ],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41105 rows in the deduplicated dataset.\n",
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1. \n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 34741\n",
      "Number of rows in validation set: 1930\n",
      "Number of rows in test set: 1931\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.GR', 'cs.CR', 'math.OC', 'eess.SP', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'eess.SY', 'cs.MA', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'stat.AP', 'cs.CY', 'stat.ME', 'stat.TH', 'math.ST', 'eess.AS', 'cs.DS', 'cs.SD', 'q-bio.QM', 'q-bio.NC', 'stat.CO', 'cs.CG', 'cs.NI', 'cs.GT', 'math.NA', 'cs.SE', 'cs.NA', 'I.2.6', 'physics.chem-ph', 'cs.DB', 'physics.comp-ph', 'cond-mat.dis-nn', 'q-bio.BM', 'math.PR', 'cs.PL', 'cs.LO', '68T45', 'cs.AR', 'physics.data-an', 'quant-ph', 'I.2.10', 'cs.CE', 'cond-mat.stat-mech', 'q-fin.ST', 'physics.ao-ph', 'I.4.6', 'math.DS', 'cs.CC', '68T05', 'physics.soc-ph', 'physics.med-ph', 'cs.PF', 'econ.EM', 'cs.DM', 'I.4.8', 'q-bio.GN', 'astro-ph.IM', 'physics.flu-dyn', 'math.AT', 'hep-ex', 'I.4', '68U10', 'q-fin.TR', 'physics.geo-ph', 'cs.FL', 'I.5.4', 'I.2', 'cond-mat.mtrl-sci', 'I.4.9', '68T10', 'physics.optics', 'I.4; I.5', '68T07', 'q-fin.CP', 'math.AP', 'I.2.6; I.2.8', 'I.2.10; I.4; I.5', '65D19', 'q-bio.PE', 'physics.app-ph', 'nlin.CD', 'math.CO', 'cs.MS', 'I.4.5', 'I.2.6; I.5.1', 'I.2.0; I.2.6', '68U01', '68T01', 'hep-ph', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68T30', '68', 'q-fin.GN', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.0', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.plasm-ph', 'physics.class-ph', 'physics.bio-ph', 'nlin.AO', 'math.SP', 'math.MP', 'math.LO', 'math.FA', 'math-ph', 'cs.DL', 'cond-mat.soft', 'I.5.2', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.1', 'I.3.7', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '60L10, 60L20', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.LG', 'stat.ML']\n",
      "Label-binarized representation: [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[3]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Deep convolutional neural networks (CNNs) have been successful in many tasks\\nin machine vision, however, millions of weights in the form of thousands of\\nconvolutional filters in CNNs makes them difficult for human intepretation or\\nunderstanding in science. In this article, we introduce CAR, a greedy\\nstructural compression scheme to obtain smaller and more interpretable CNNs,\\nwhile achieving close to original accuracy. The compression is based on pruning\\nfilters with the least contribution to the classification accuracy. We\\ndemonstrate the interpretability of CAR-compressed CNNs by showing that our\\nalgorithm prunes filters with visually redundant functionalities such as color\\nfilters. These compressed networks are easier to interpret because they retain\\nthe filter diversity of uncompressed networks with order of magnitude less\\nfilters. Finally, a variant of CAR is introduced to quantify the importance of\\neach image category to each CNN filter. Specifically, the most and the least\\nimportant class labels are shown to be meaningful interpretations of each\\nfilter.'\n",
      "Label(s): ['cs.CV']\n",
      "Abstract: b'Importance: Lung cancer is the leading cause of cancer mortality in the US,\\nresponsible for more deaths than breast, prostate, colon and pancreas cancer\\ncombined and it has been recently demonstrated that low-dose computed\\ntomography (CT) screening of the chest can significantly reduce this death\\nrate.\\n  Objective: To compare the performance of a deep learning model to\\nstate-of-the-art automated algorithms and radiologists as well as assessing the\\nrobustness of the algorithm in heterogeneous datasets.\\n  Design, Setting, and Participants: Three low-dose CT lung cancer screening\\ndatasets from heterogeneous sources were used, including National Lung\\nScreening Trial (NLST, n=3410), Lahey Hospital and Medical Center (LHMC,\\nn=3174) data, Kaggle competition data (from both stages, n=1595+505) and the\\nUniversity of Chicago data (UCM, a subset of NLST, annotated by radiologists,\\nn=197). Relevant works on automated methods for Lung Cancer malignancy\\nestimation have used significantly less data in size and diversity. At the\\nfirst stage, our framework employs a nodule detector; while in the second\\nstage, we use both the image area around the nodules and nodule features as\\ninputs to a neural network that estimates the malignancy risk for the entire CT\\nscan. We trained our two-stage algorithm on a part of the NLST dataset, and\\nvalidated it on the other datasets.\\n  Results, Conclusions, and Relevance: The proposed deep learning model: (a)\\ngeneralizes well across all three data sets, achieving AUC between 86% to 94%;\\n(b) has better performance than the widely accepted PanCan Risk Model,\\nachieving 11% better AUC score; (c) has improved performance compared to the\\nstate-of-the-art represented by the winners of the Kaggle Data Science Bowl\\n2017 competition on lung cancer screening; (d) has comparable performance to\\nradiologists in estimating cancer risk at a patient level.'\n",
      "Label(s): ['cs.CV']\n",
      "Abstract: b'Detection of small objects in large swaths of imagery is one of the primary\\nproblems in satellite imagery analytics. While object detection in ground-based\\nimagery has benefited from research into new deep learning approaches,\\ntransitioning such technology to overhead imagery is nontrivial. Among the\\nchallenges is the sheer number of pixels and geographic extent per image: a\\nsingle DigitalGlobe satellite image encompasses >64 km2 and over 250 million\\npixels. Another challenge is that objects of interest are minuscule (often only\\n~10 pixels in extent), which complicates traditional computer vision\\ntechniques. To address these issues, we propose a pipeline (You Only Look\\nTwice, or YOLT) that evaluates satellite images of arbitrary size at a rate of\\n>0.5 km2/s. The proposed approach can rapidly detect objects of vastly\\ndifferent scales with relatively little training data over multiple sensors. We\\nevaluate large test images at native resolution, and yield scores of F1 > 0.8\\nfor vehicle localization. We further explore resolution and object size\\nrequirements by systematically testing the pipeline at decreasing resolution,\\nand conclude that objects only ~5 pixels in size can still be localized with\\nhigh confidence. Code is available at https://github.com/CosmiQ/yolt.'\n",
      "Label(s): ['cs.CV']\n",
      "Abstract: b'Metric learning especially deep metric learning has been widely developed for\\nlarge-scale image inputs data. However, in many real-world applications, we can\\nonly have access to vectorized inputs data. Moreover, on one hand, well-labeled\\ndata is usually limited due to the high annotation cost. On the other hand, the\\nreal data is commonly streaming data, which requires to be processed online. In\\nthese scenarios, the fashionable deep metric learning is not suitable anymore.\\nTo this end, we reconsider the traditional shallow online metric learning and\\nnewly develop an online progressive deep metric learning (ODML) framework to\\nconstruct a metric-algorithm-based deep network. Specifically, we take an\\nonline metric learning algorithm as a metric-algorithm-based layer (i.e.,\\nmetric layer), followed by a nonlinear layer, and then stack these layers in a\\nfashion similar to deep learning. Different from the shallow online metric\\nlearning, which can only learn one metric space (feature transformation), the\\nproposed ODML is able to learn multiple hierarchical metric spaces.\\nFurthermore, in a progressively and nonlinearly learning way, ODML has a\\nstronger learning ability than traditional shallow online metric learning in\\nthe case of limited available training data. To make the learning process more\\nexplainable and theoretically guaranteed, we also provide theoretical analysis.\\nThe proposed ODML enjoys several nice properties and can indeed learn a metric\\nprogressively and performs better on the benchmark datasets. Extensive\\nexperiments with different settings have been conducted to verify these\\nproperties of the proposed ODML.'\n",
      "Label(s): ['cs.CV' 'cs.LG']\n",
      "Abstract: b'Neural networks-based learning of the distribution of non-dispatchable\\nrenewable electricity generation from sources such as photovoltaics (PV) and\\nwind as well as load demands has recently gained attention. Normalizing flow\\ndensity models have performed particularly well in this task due to the\\ntraining through direct log-likelihood maximization. However, research from the\\nfield of image generation has shown that standard normalizing flows can only\\nlearn smeared-out versions of manifold distributions and can result in the\\ngeneration of noisy data. To avoid the generation of time series data with\\nunrealistic noise, we propose a dimensionality-reducing flow layer based on the\\nlinear principal component analysis (PCA) that sets up the normalizing flow in\\na lower-dimensional space. We train the resulting principal component flow\\n(PCF) on data of PV and wind power generation as well as load demand in Germany\\nin the years 2013 to 2015. The results of this investigation show that the PCF\\npreserves critical features of the original distributions, such as the\\nprobability density and frequency behavior of the time series. The application\\nof the PCF is, however, not limited to renewable power generation but rather\\nextends to any data set, time series, or otherwise, which can be efficiently\\nreduced using PCA.'\n",
      "Label(s): ['cs.LG']\n"
     ]
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(vocab, hot_indices)\n",
    "\n",
    "\n",
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    # print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159027\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text vectorization\n",
    "# Initializes a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our training set.\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "272/272 [==============================] - 420s 2s/step - loss: 0.0490 - binary_accuracy: 0.9830 - val_loss: 0.0183 - val_binary_accuracy: 0.9947\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - 391s 1s/step - loss: 0.0174 - binary_accuracy: 0.9950 - val_loss: 0.0184 - val_binary_accuracy: 0.9946\n",
      "Epoch 3/20\n",
      "272/272 [==============================] - 1466s 5s/step - loss: 0.0134 - binary_accuracy: 0.9960 - val_loss: 0.0184 - val_binary_accuracy: 0.9947\n",
      "Epoch 4/20\n",
      "272/272 [==============================] - 451s 2s/step - loss: 0.0110 - binary_accuracy: 0.9967 - val_loss: 0.0190 - val_binary_accuracy: 0.9946\n",
      "Epoch 5/20\n",
      "272/272 [==============================] - 339s 1s/step - loss: 0.0094 - binary_accuracy: 0.9972 - val_loss: 0.0195 - val_binary_accuracy: 0.9946\n",
      "Epoch 6/20\n",
      "272/272 [==============================] - 1111s 4s/step - loss: 0.0083 - binary_accuracy: 0.9975 - val_loss: 0.0201 - val_binary_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Creating shallow_mlp_model (MLP) with dropout layers\n",
    "model1 = keras.Sequential([\n",
    "    # First hidden layer: 512 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Second hidden layer: 256 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Output layer: The number of neurons equals the vocabulary size (output vocabulary of the StringLookup layer), with a sigmoid activation function.\n",
    "    layers.Dense(lookup.vocabulary_size(), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\REDMI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the model\n",
    "model1.save(\"models/model.h5\")\n",
    "\n",
    "# Save the configuration of the text vectorizer\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_text_vectorizer_config, f)\n",
    "\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration of the text vectorizer\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "    saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer with the saved configuration\n",
    "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)\n",
    "\n",
    "# with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "#     weights = pickle.load(f)\n",
    "#     loaded_text_vectorizer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for recommendation system\n",
    "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.drop_duplicates(inplace= True)\n",
    "arxiv_data.reset_index(drop= True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence transformer\n",
    "!pip install -U -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970c3d57cf0c4b90bf3c4abb78edc674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92982d954a545fe97f7a354cedbaa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849ed51cdf6c402694016d80dfa47482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c4ce256a654c09855fb26212d470e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471c7f55d766494fbbff3ad415800dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab2559480ed441dba61d1d749e3e8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5466efd3d1994b0d8056d40b55e7edad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7f9ead493d49c099b9528715bd093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be49433ffefb4684a413583cfc9d881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514b25a3796748da8a185cfee3f972aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12d46e138eb4b95ae4fd797e0d20d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922a55a237984058b35eacf3eb52ccdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d8ffcb074f45b0b5a461ad8f4d9fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e7e27befb34f9dbb1b3f64892c2b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# we load all-MiniLM-L6-v2, which is a MiniLM model fine tuned on a large dataset of over \n",
    "# 1 billion training pairs.\n",
    "#This initializes the 'all-MiniLM-L6-v2' model from Sentence Transformers. \n",
    "# This model is capable of encoding sentences into fixed-size vectors (embeddings).\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#Our sentences we like to encode\n",
    "sentences = arxiv_data['titles']\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41105, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Lifelong Graph Learning\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "#This loop iterates over pairs of sentences and their corresponding embeddings. \n",
    "#zip is used to iterate over both lists simultaneously.\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    print(\"\")\n",
    "    # Breaks out of the loop after printing information for the first 5 sentences.\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open('models/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "with open('models/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "with open('models/rec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOMMEND SiMILAR PAPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save files\n",
    "embeddings = pickle.load(open('models/embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('models/sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('models/rec_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, rec_model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "Attention that does not Explain Away\n",
      "Area Attention\n",
      "Pay Attention when Required\n",
      "Long Short-Term Attention\n",
      "Attention as Activation\n"
     ]
    }
   ],
   "source": [
    "input_paper = input(\"Enter the title of any paper you like\")\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cpu\n",
      "2.2.2\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# install this versions\n",
    "import sentence_transformers\n",
    "import tensorflow\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
